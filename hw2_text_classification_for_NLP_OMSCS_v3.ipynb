{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"JAEWON LEE\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywvLShfXIKS1"
   },
   "source": [
    "# NLP Classification\n",
    "\n",
    "In this assignment we look at several ways of classifying texts:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Multinomial Regression\n",
    "\n",
    "We also look at binary label classification problems (e.g., sentiment analysis) and multinomial classification problems (e.g., topic analysis).\n",
    "\n",
    "We will use two datasets:\n",
    "- [IMDb movie review sentiment](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- [AG News topics](https://huggingface.co/datasets/ag_news)\n",
    "\n",
    "**Tips:**\n",
    "- Read all the code. We don't ask you to write the training loops, or evaluation loops, but it is often instructive to see how the models are trained and evaluated.\n",
    "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMRiPTv7vHe2"
   },
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xVv6McXodZG"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S5VeRBOiIKS4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# This will allow visualization of the neural network structure\n",
    "# !pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8r3ho388zST"
   },
   "source": [
    "Check if GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mws7jUg9oh_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyRo0v0JIKTK"
   },
   "source": [
    "# Functions for cleaning up raw texts and tokenizing the corpus\n",
    "\n",
    "We perform text preprocessing that includes: removing HTML tags, making text lower case, stemming, and disposing of stopwords. \n",
    "In the end, we will split the entire dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JvfvVWHyIKTL"
   },
   "outputs": [],
   "source": [
    "# Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wr_LdWWoIKTL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "print(stopwords_english)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list):\n",
    "    tokens = [token.strip() for token in text]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NCXHv_tZIKTM"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_clean(line, stem_and_remove_stop_words = True):\n",
    "\n",
    "    line = re.sub(r\"<.*?>\", \"\", line).strip() # remove all HTML tags \n",
    "    line = re.sub(r'[^a-zA-Z0-9]', ' ', line) # remove punc\n",
    "    line = line.lower().split()  # lower case\n",
    "    if stem_and_remove_stop_words:\n",
    "        line = remove_stopwords(line, stopwords_english)\n",
    "        line = simple_stemmer(line)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohc6u_lDIKS-"
   },
   "source": [
    "# Download and unpack the sentiment data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIklHox9vjMI"
   },
   "source": [
    "We are using IMDb Dataset for binary sentiment classification that provides a set of 25K highly polar reviews for training, and 25K for testing \n",
    "(each set contains an equal number of positive and negative examples). \n",
    "\n",
    "Dataset folder structure is as follows:\n",
    "\n",
    "dataset/ \\\n",
    "├── test/ \\\n",
    "│     ├── pos/ \\\n",
    "│     ├── neg/ \\\n",
    "├── train/ \\\n",
    "      ├── pos/ \\\n",
    "      └── neg/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BqHNeua_IKS_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!gunzip aclImdb_v1.tar.gz\n",
    "!tar -xvf aclImdb_v1.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRD03KapvnI0"
   },
   "source": [
    "Load in the text from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HI3VdrTnIKTO"
   },
   "outputs": [],
   "source": [
    "def load_text_from_folders(path, file_list, dataset, samples = 1000, stem_and_remove_stop_words = True):\n",
    "    \"\"\"Read set of files from given directory and save returned lines to list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Absolute or relative path to given file (or set of files).\n",
    "    file_list: list\n",
    "        List of files names to read.\n",
    "    dataset: list\n",
    "        List that stores read lines.\n",
    "    samples: int\n",
    "        Number of samples in the output\n",
    "    \"\"\"\n",
    "    for i, file in enumerate(file_list):\n",
    "        if i >= samples:\n",
    "            break\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf8') as text:\n",
    "            contents = text.read()\n",
    "            contents_tokenized = tokenize_and_clean(contents, stem_and_remove_stop_words=stem_and_remove_stop_words)\n",
    "            dataset.append(contents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8KuCNoXIKTO"
   },
   "source": [
    "# Creating training and test sets\n",
    "\n",
    "This creates four arrays: \n",
    "\n",
    "\n",
    "*   ```train_pos``` -- instances in the training set with positive sentiment labels\n",
    "*   ```train_neg``` -- instances in the training set with negative sentiment labels\n",
    "*   ```test_pos``` -- instances in the testing set with positive sentiment labels\n",
    "*   ```test_neg``` -- instances in the testing set with negative sentiment labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4Nv5FvBIIKTO"
   },
   "outputs": [],
   "source": [
    "# Path to dataset location\n",
    "path = 'aclImdb/'\n",
    "\n",
    "# Create lists that will contain read lines\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "\n",
    "# Create a dictionary of paths and lists that store lines (key: value = path: list)\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg,\n",
    "             'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "\n",
    "# Load the data\n",
    "for dataset in sets_dict:\n",
    "  file_list = [f for f in os.listdir(os.path.join(path, dataset)) if f.endswith('.txt')]\n",
    "  load_text_from_folders(os.path.join(path, dataset), file_list, sets_dict[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t5nCE_xpvx7"
   },
   "source": [
    "Convert into Pandas dataframes. Pandas is a virtual spreadsheet with a programmatic API. A ```DataFrame``` is a spreadsheet. We will make a spreadsheet of training data and one for testing data and one with everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kovNgXqIIKTP"
   },
   "outputs": [],
   "source": [
    "# Concatenate training and testing examples into one dataset\n",
    "TRAIN = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}),\n",
    "                     pd.DataFrame({'review': train_neg, 'label':0})],\n",
    "                     axis=0, ignore_index=True)\n",
    "\n",
    "TEST = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}),\n",
    "                    pd.DataFrame({'review': test_neg, 'label':0})],\n",
    "                    axis=0, ignore_index=True)\n",
    "\n",
    "ALL = pd.concat([TRAIN, TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKUFtqERp79N"
   },
   "source": [
    "Look at the data. \n",
    "\n",
    "This is a summary of the data. We see that the data is balanced between labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0xPnt_CDIKTP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1000\n",
       "0    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlV_TMBLqOYB"
   },
   "source": [
    "This is the first few rows of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "InVC2IvnIKTQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typic, mel, brook, film, much, less, slapstic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
       "1  [homeless, houseless, georg, carlin, state, is...      1\n",
       "2  [brilliant, act, lesley, ann, warren, best, dr...      1\n",
       "3  [easili, underr, film, inn, brook, cannon, sur...      1\n",
       "4  [typic, mel, brook, film, much, less, slapstic...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jP5U6usIKTR"
   },
   "source": [
    "# Creating a vocabulary file \n",
    "\n",
    "Next, we have to build a vocabulary. This is effectively a look-up table where every unique word in your data set has a corresponding index (an integer).\n",
    "We do this as our machine learning model cannot operate on strings, but only numbers. Each index is used to construct a one-hot vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Oz6hQlCgqckU"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._word2index = {}\n",
    "        self._word2count = {}\n",
    "        self._index2word = {}\n",
    "        self._n_words = 0  \n",
    "\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT7oI7pXw9wA"
   },
   "source": [
    "Make a vocab object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-FLxtGNgw79Q"
   },
   "outputs": [],
   "source": [
    "VOCAB = Vocab(\"imdb\")\n",
    "VOCAB_SIZE = 1000\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggqgPtCSxAG-"
   },
   "source": [
    "Load the first ```n``` frequent words in the vocabulary. Do this by sorting by frequency and then truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DAiKNELzwC2T"
   },
   "outputs": [],
   "source": [
    "# Get word frequency counts\n",
    "word_freq_dict = {}   # key = word, value = frequency\n",
    "for review in ALL['review']:\n",
    "  for word in review:\n",
    "    if word in word_freq_dict:\n",
    "      word_freq_dict[word] += 1\n",
    "    else:\n",
    "      word_freq_dict[word] = 1\n",
    "\n",
    "# Get a list of (word, freq) tuples sorted by frequency\n",
    "kv_list = []  # list of word-freq tuples so can sort\n",
    "for (k,v) in word_freq_dict.items():\n",
    "  kv_list.append((k,v))\n",
    "sorted_kv_list = sorted(kv_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load top n words in to vocab object\n",
    "for word, freq in sorted_kv_list[:VOCAB_SIZE]:\n",
    "  VOCAB.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvh0yHT_IKTi"
   },
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes Algorithm is based on the Bayes Rule which describes the probability of an event, \n",
    "based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "According to Bayes theorem:\n",
    "\n",
    "\n",
    "```Posterior = likelihood * proposition/evidence```\n",
    "\n",
    "or\n",
    "\n",
    "```P(A|B) = P(B|A) * P(A)/P(B)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vELAfeWDRrNq"
   },
   "source": [
    "Using word presence as features, create an array of features for each review. Each review will thus be an array of size ```len(vocab)``` where each index in the array is a token number and the value in that position is whether the token is present in the review. There will be ```num_rows``` arrays, making a ```num_rows x len(vocab)``` 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFf6zYmozBtc"
   },
   "source": [
    "This function creates a bag of words. It returns a vector where each element is a count of the words in the sentence corresponding to the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DKCwfenwIKTw"
   },
   "outputs": [],
   "source": [
    "def make_bow(sentence):\n",
    "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64)\n",
    "    for word in sentence:\n",
    "        if word not in VOCAB.get_words():\n",
    "            continue\n",
    "        vec[VOCAB.word2index(word)] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHweI8gPzOdh"
   },
   "source": [
    "Prepare data ```X_TRAIN``` is a 2D array of size ```num_reviews x vocab_size``` that contains training data. Each row will be a bag of words, except each index contains a 1 or 0 based on word presence in the example. Each row is a vector of features $\\phi_1 ... \\phi_{|V|}$ assumed to be independent, where $|V|$ is size of the vocabulary. We don't need to know what the features are, only whether they are present in each example in the training set.\n",
    "\n",
    "```X_TEST``` is the same as above but containing testing data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "c30F5ZwyIKTw"
   },
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers \n",
    "# Make empty vectors\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "# Load in frequency counts\n",
    "for i, row in TRAIN.iterrows():\n",
    "    X_TRAIN[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "    X_TEST[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "# The labels\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao16p7_U7Zdz"
   },
   "source": [
    "What you want to do is to compute probabilities over the training data and then apply those probabilities to the testing examples. Use the Bayes formula to compute $P_{\\rm test}(L_{+}|\\phi_{0:|V|})$ and $P_{\\rm test}(L_{-}|\\phi_{0:|V|})$ for each review. Classify examples based on whether one probability is higher than another. That is, $sign(P_{\\rm test}(L_{+}|\\phi_{0:|V|}) - P_{\\rm test}(L_{-}|\\phi_{0:|V|}))$ indicates a positive review when greater than 0 and a negative review when less than 0.\n",
    "\n",
    "**Hint:** You do not need to implement any loops. Numpy indexing and slicing operations, along with built in functions like `.mean()`, `.sum()`, etc. will allow all operations to be performed on each row of the data in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2ch6U7uNmeN"
   },
   "source": [
    "Step 1: Compute the positive label condition:\n",
    "$P(L_{+}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{+})P(L_{+}) / P(\\phi_{0:|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "id": "AwAAV9Zq7YbI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92849a5596ef346c7f7d64246442952c",
     "grade": false,
     "grade_id": "cell-4e927fe9f8caf96a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_pos_given_features(x_train, y_train):\n",
    "  log_probs = np.array([0] * x_train.shape[1]) \n",
    "  # YOUR CODE HERE\n",
    "  log_probs = np.log((x_train[y_train == 1].sum(axis=0) + 1) / (x_train[y_train == 1].sum() + x_train.shape[1]))\n",
    "  \n",
    "  return log_probs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eeKepgkNpNa"
   },
   "source": [
    "Step 2: Compute the negative label condition:\n",
    "$P(L_{-}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{-})P(L_{-}) / P(\\phi_{0:|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "nWP9U3BuMess",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae513d49bd65a8146848e5f5aa9119da",
     "grade": false,
     "grade_id": "cell-996c909da2d8b8bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_neg_given_features(x_train, y_train):\n",
    "  log_probs = np.array([0] * x_train.shape[1]) \n",
    "  # YOUR CODE HERE\n",
    "  log_probs = np.log((x_train[y_train == 0].sum(axis=0) + 1) / (x_train[y_train == 0].sum() + x_train.shape[1]))\n",
    "  return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZXdgx0Ku9sYq"
   },
   "outputs": [],
   "source": [
    "pos_probs = prob_pos_given_features(X_TRAIN, Y_TRAIN)\n",
    "neg_probs = prob_neg_given_features(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9wbtxDZNuj3"
   },
   "source": [
    "Step 3: Make a label prediction. Subtract (in log scale) the positive from the negative. If the result is greater than zero then it is a prediction of `+` label. If the result is less thn zero then we make a prediction of `-` label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "id": "xaz7KBdd_CD1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46c358117130ab2c03ed64eb2319b1b4",
     "grade": false,
     "grade_id": "cell-7b4cde57fd68ac07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_bayes(x, pos_probs, neg_probs):\n",
    "  label = 0\n",
    "  # YOUR CODE HERE\n",
    "  label = (x * pos_probs).sum() - (x * neg_probs).sum()\n",
    "  if label > 0:\n",
    "    label = 1\n",
    "  else:\n",
    "    label = 0\n",
    "    \n",
    "  return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOFzFVU_is3"
   },
   "source": [
    "# Naive Bayes Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Lr3PgUnxPYet"
   },
   "outputs": [],
   "source": [
    "def test_naive_bayes(x_train, y_train, x_test, y_test):\n",
    "  # Get the positive and negative feature probabilities\n",
    "  pos_probs = prob_pos_given_features(x_train, y_train)\n",
    "  neg_probs = prob_neg_given_features(x_train, y_train)\n",
    "  correct = 0 # How many tests are correct\n",
    "  # Iterate through the test set\n",
    "  for x, y in zip(x_test, y_test):\n",
    "    # Get the naive_bayes label\n",
    "    label = naive_bayes(x, pos_probs, neg_probs)\n",
    "    # Compare the label against the true label\n",
    "    correct = correct + int(label == y)\n",
    "  return correct / x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T1rcPgnWP_6V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8525"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student check - accuracies >= 80% will receive full credit (no credit for less than 80%)\n",
    "test_naive_bayes(X_TRAIN, Y_TRAIN, X_TEST, Y_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AXcKoWBiO35Q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a29d4009b7f9e2ae71376d5c6f91b731",
     "grade": true,
     "grade_id": "cell-d47a2bcfa7277d64",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUTymITZIKT5"
   },
   "source": [
    "# Logistic Regression - Part 1\n",
    "\n",
    "We will be using a neural network to perform logistic regression. We will use word counts as the input feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWKJNb2CXhe5"
   },
   "source": [
    "Reload the data, but use word counts instead of word presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sBjsMbDSIKT6"
   },
   "outputs": [],
   "source": [
    "# Randomize the data\n",
    "TRAIN = TRAIN.sample(frac=1).reset_index(drop=True)\n",
    "TEST = TEST.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Vectorize text reviews to numbers\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "for i, row in TRAIN.iterrows():\n",
    "  X_TRAIN[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "  X_TEST[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSBC93Ge6F1e"
   },
   "source": [
    "Make a logistic classifier torch neural network.\n",
    "\n",
    "Complete the constructor and forward function. The net will take an arbitrary number of outputs, but for binary logistic regression, only one is needed because the single output neuron can take a value that is between 0 and 1, with 0 meaning negative sentiment and 1 meaning positive sentiment. There should only be as many parameters as ```num_features x (num_labels-1)``` in binary logistic regression and ```num_features x num_labels``` for multinomial logistic regression.\n",
    "\n",
    "The input will be a one-hot vector of size `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "yeJnVl7NIKT8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba378ab9d657819e1adceeb09edaf545",
     "grade": false,
     "grade_id": "cell-c6124df74ac9d8f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "  def __init__(self, num_labels, vocab_size):\n",
    "    super(BoWClassifier, self).__init__()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "  def forward(self, bow_vec): \n",
    "    # YOUR CODE HERE\n",
    "    logits=self.linear(bow_vec)\n",
    "    out = torch.sigmoid(logits)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuxMl7K26Q88"
   },
   "source": [
    "Make the model, move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mgPYvkocIKT8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (linear): Linear(in_features=1000, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "# Use one label because the head can signify a 1 or 0 because of the sigmoid. \n",
    "bow_nn_model = BoWClassifier(NUM_LABELS-1, VOCAB_SIZE)\n",
    "bow_nn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaBdfHfDQ5oI"
   },
   "source": [
    "This function should return two tensors. The first, containing training data, shoud be of size ```batch_size x vocab_size``` for the ```i```th batch. The second should be a list of labels of size ```batch_size```. Both tensors should be of type ```dtype=torch.float```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "id": "RPxjUwUTPZTq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a4980dd49cb5893ebfeeea42605a3e",
     "grade": false,
     "grade_id": "cell-abf634c3a00755c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, x_data, y_data):\n",
    "  # Make some empty tensors\n",
    "  x = torch.zeros([batch_size, x_data.shape[1]])\n",
    "  y = torch.zeros([batch_size, 1]) \n",
    "  # YOUR CODE HERE\n",
    "  x = torch.tensor(x_data[i:i+batch_size,:], dtype=torch.float)\n",
    "  y = torch.tensor(y_data[i:i+batch_size], dtype=torch.float)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eMh98LwOwv_"
   },
   "source": [
    "# Logistic Regression - Part 1 Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "muyKFe6tRevo",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a785d07a82ee1ef92daf307f4a29285",
     "grade": true,
     "grade_id": "cell-f5c3c6db8f3f380d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tp-B0qeXNnyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1000])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "batch_index = 0\n",
    "batch_size = 1000\n",
    "x, y = get_batch(batch_index, batch_size, X_TRAIN, Y_TRAIN)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jsZqfZymZ5Ad",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddc488f9413d32fd63049b83398d1934",
     "grade": true,
     "grade_id": "cell-1dcd3ba66473b255",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FdPVSLtrN_2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has one layer as expected!\n"
     ]
    }
   ],
   "source": [
    "# student check - your model must have the expected number of layers to receive full credit, no credit otherwise\n",
    "if len(list(bow_nn_model.children())) != 1:\n",
    "  print(\"Model does not have the expected number of layers.\")\n",
    "else:\n",
    "  print(\"Your model has one layer as expected!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Qpdx8wbRaLGe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2573b22da0fbdfb9f430f734860b665",
     "grade": true,
     "grade_id": "cell-4316b749692a676b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FOYrmRVUj7g7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output shape looks good!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"249pt\" height=\"403pt\"\n",
       " viewBox=\"0.00 0.00 249.00 403.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 399)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-399 245,-399 245,4 -4,4\"/>\n",
       "<!-- 2115422139344 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2115422139344</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"165,-31 106,-31 106,0 165,0 165,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1000)</text>\n",
       "</g>\n",
       "<!-- 2115422182304 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2115422182304</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"143,-92 0,-92 0,-73 143,-73 143,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.5\" y=\"-80\" font-family=\"monospace\" font-size=\"10.00\">ReshapeAliasBackward0</text>\n",
       "</g>\n",
       "<!-- 2115422182304&#45;&gt;2115422139344 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2115422182304&#45;&gt;2115422139344</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.13,-72.73C88.75,-63.98 102.35,-50.17 113.91,-38.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116.62,-40.67 121.14,-31.08 111.63,-35.75 116.62,-40.67\"/>\n",
       "</g>\n",
       "<!-- 2115422190848 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2115422190848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"192,-153 79,-153 79,-134 192,-134 192,-153\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.5\" y=\"-141\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n",
       "</g>\n",
       "<!-- 2115422190848&#45;&gt;2115422182304 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2115422190848&#45;&gt;2115422182304</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.06,-133.79C116.23,-124.73 100.61,-110.34 88.61,-99.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.71,-96.45 80.98,-92.24 85.96,-101.59 90.71,-96.45\"/>\n",
       "</g>\n",
       "<!-- 2115422141104 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2115422141104</title>\n",
       "<polygon fill=\"#a2cd5a\" stroke=\"black\" points=\"238,-98 161,-98 161,-67 238,-67 238,-98\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\"> (1000, 1)</text>\n",
       "</g>\n",
       "<!-- 2115422190848&#45;&gt;2115422141104 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2115422190848&#45;&gt;2115422141104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.94,-133.79C153.11,-126.26 165.27,-115.05 176.04,-105.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178.58,-107.54 183.56,-98.19 173.84,-102.4 178.58,-107.54\"/>\n",
       "</g>\n",
       "<!-- 2115422183120 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2115422183120</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"186,-208 85,-208 85,-189 186,-189 186,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.5\" y=\"-196\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2115422183120&#45;&gt;2115422190848 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2115422183120&#45;&gt;2115422190848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.5,-188.75C135.5,-181.8 135.5,-171.85 135.5,-163.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139,-163.09 135.5,-153.09 132,-163.09 139,-163.09\"/>\n",
       "</g>\n",
       "<!-- 2115422186336 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2115422186336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"132,-263 31,-263 31,-244 132,-244 132,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2115422186336&#45;&gt;2115422183120 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2115422186336&#45;&gt;2115422183120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.42,-243.75C98.28,-236.03 109.93,-224.6 119.42,-215.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.06,-217.59 126.75,-208.09 117.16,-212.6 122.06,-217.59\"/>\n",
       "</g>\n",
       "<!-- 2115413141728 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2115413141728</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"122,-329 39,-329 39,-299 122,-299 122,-329\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.5\" y=\"-317\" font-family=\"monospace\" font-size=\"10.00\">linear.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"80.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 2115413141728&#45;&gt;2115422186336 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2115413141728&#45;&gt;2115422186336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.74,-298.84C80.87,-291.21 81.03,-281.7 81.18,-273.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.68,-273.32 81.35,-263.27 77.68,-273.2 84.68,-273.32\"/>\n",
       "</g>\n",
       "<!-- 2115422180048 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2115422180048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"228,-263 151,-263 151,-244 228,-244 228,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2115422180048&#45;&gt;2115422183120 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2115422180048&#45;&gt;2115422183120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.58,-243.75C172.72,-236.03 161.07,-224.6 151.58,-215.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"153.84,-212.6 144.25,-208.09 148.94,-217.59 153.84,-212.6\"/>\n",
       "</g>\n",
       "<!-- 2115422191040 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2115422191040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"241,-323.5 140,-323.5 140,-304.5 241,-304.5 241,-323.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-311.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2115422191040&#45;&gt;2115422180048 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2115422191040&#45;&gt;2115422180048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M190.35,-304.37C190.21,-296.25 190,-283.81 189.82,-273.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.32,-273.11 189.65,-263.17 186.32,-273.23 193.32,-273.11\"/>\n",
       "</g>\n",
       "<!-- 2115402936816 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2115402936816</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"238,-395 143,-395 143,-365 238,-365 238,-395\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-383\" font-family=\"monospace\" font-size=\"10.00\">linear.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-372\" font-family=\"monospace\" font-size=\"10.00\"> (1, 1000)</text>\n",
       "</g>\n",
       "<!-- 2115402936816&#45;&gt;2115422191040 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2115402936816&#45;&gt;2115422191040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M190.5,-364.8C190.5,-355.7 190.5,-343.79 190.5,-333.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194,-333.84 190.5,-323.84 187,-333.84 194,-333.84\"/>\n",
       "</g>\n",
       "<!-- 2115422141104&#45;&gt;2115422139344 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2115422141104&#45;&gt;2115422139344</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M184.98,-66.75C176.71,-58.36 166.2,-47.68 157,-38.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159.48,-35.86 149.97,-31.19 154.49,-40.77 159.48,-35.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1ec88fcfb20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student check\n",
    "y_hat = bow_nn_model(x.to(device))\n",
    "y_hat = y_hat.reshape(-1)\n",
    "\n",
    "''' Hidden auto-grader test - do not delete this cell '''\n",
    "if y_hat.shape[0] == batch_size:\n",
    "  print(\"Forward pass output shape looks good!\")\n",
    "else:\n",
    "  print(\"Seems to be an issue with output shape in your model...\")\n",
    "make_dot(y_hat, dict(bow_nn_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN5v9cjKif_H"
   },
   "source": [
    "# Logistic Regression - Part 2\n",
    "\n",
    "Create a dataset as an array of (X_train, label).\n",
    "\n",
    "Complete ```get_batch(i)``` and set ```batch_size``` and ```num_epochs```.\n",
    "\n",
    "Training loop will call ```get_batch()``` with the iteration number and do everything else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "paJEFuigIKT-"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_data, test_data, epochs, batch_size):\n",
    "  n_iter = len(train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "  # Loss Function\n",
    "  loss_function = nn.BCELoss()\n",
    "  # Optimizer initlialization\n",
    "  optimizer = optim.SGD(bow_nn_model.parameters(), lr=0.1)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # Make BOW vector for input features and target label\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, train_data, test_data)\n",
    "\n",
    "      # Step 3. Run the forward pass.\n",
    "      y_hat = model(x.to(device))\n",
    "      y_hat = y_hat.reshape(-1)\n",
    "\n",
    "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "      loss = loss_function(y_hat,y.to(device))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(torch.round(y_hat)==y.to(device)).float().mean())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gzD90s3KQgJd"
   },
   "outputs": [],
   "source": [
    "# It's ok to modify this cell.\n",
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Tv8Lqg8T949T",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 batches per epoch\n",
      "epoch: 10 ,loss = 0.5439162850379944 , training accuracy = tensor(0.8180)\n",
      "epoch: 20 ,loss = 0.46669602394104004 , training accuracy = tensor(0.8670)\n",
      "epoch: 30 ,loss = 0.41843506693840027 , training accuracy = tensor(0.8890)\n",
      "epoch: 40 ,loss = 0.3839007019996643 , training accuracy = tensor(0.9010)\n",
      "epoch: 50 ,loss = 0.3572941720485687 , training accuracy = tensor(0.9050)\n",
      "epoch: 60 ,loss = 0.3358120918273926 , training accuracy = tensor(0.9100)\n",
      "epoch: 70 ,loss = 0.3178950846195221 , training accuracy = tensor(0.9160)\n",
      "epoch: 80 ,loss = 0.30259114503860474 , training accuracy = tensor(0.9230)\n",
      "epoch: 90 ,loss = 0.2892790138721466 , training accuracy = tensor(0.9320)\n",
      "epoch: 100 ,loss = 0.27753227949142456 , training accuracy = tensor(0.9380)\n",
      "epoch: 110 ,loss = 0.2670460641384125 , training accuracy = tensor(0.9450)\n",
      "epoch: 120 ,loss = 0.2575954496860504 , training accuracy = tensor(0.9470)\n",
      "epoch: 130 ,loss = 0.24900978803634644 , training accuracy = tensor(0.9490)\n",
      "epoch: 140 ,loss = 0.24115689098834991 , training accuracy = tensor(0.9530)\n",
      "epoch: 150 ,loss = 0.23393204808235168 , training accuracy = tensor(0.9540)\n",
      "epoch: 160 ,loss = 0.22725123167037964 , training accuracy = tensor(0.9550)\n",
      "epoch: 170 ,loss = 0.2210458666086197 , training accuracy = tensor(0.9560)\n",
      "epoch: 180 ,loss = 0.21525929868221283 , training accuracy = tensor(0.9560)\n",
      "epoch: 190 ,loss = 0.2098442018032074 , training accuracy = tensor(0.9580)\n",
      "epoch: 200 ,loss = 0.20476077497005463 , training accuracy = tensor(0.9590)\n",
      "epoch: 210 ,loss = 0.19997502863407135 , training accuracy = tensor(0.9610)\n",
      "epoch: 220 ,loss = 0.19545790553092957 , training accuracy = tensor(0.9630)\n",
      "epoch: 230 ,loss = 0.19118426740169525 , training accuracy = tensor(0.9650)\n",
      "epoch: 240 ,loss = 0.18713225424289703 , training accuracy = tensor(0.9670)\n",
      "epoch: 250 ,loss = 0.1832827925682068 , training accuracy = tensor(0.9690)\n",
      "epoch: 260 ,loss = 0.17961907386779785 , training accuracy = tensor(0.9710)\n",
      "epoch: 270 ,loss = 0.17612624168395996 , training accuracy = tensor(0.9730)\n",
      "epoch: 280 ,loss = 0.17279107868671417 , training accuracy = tensor(0.9740)\n",
      "epoch: 290 ,loss = 0.16960187256336212 , training accuracy = tensor(0.9750)\n",
      "epoch: 300 ,loss = 0.1665480136871338 , training accuracy = tensor(0.9750)\n",
      "epoch: 310 ,loss = 0.16362008452415466 , training accuracy = tensor(0.9770)\n",
      "epoch: 320 ,loss = 0.16080954670906067 , training accuracy = tensor(0.9770)\n",
      "epoch: 330 ,loss = 0.15810859203338623 , training accuracy = tensor(0.9780)\n",
      "epoch: 340 ,loss = 0.1555102914571762 , training accuracy = tensor(0.9780)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 350 ,loss = 0.15300816297531128 , training accuracy = tensor(0.9780)\n",
      "epoch: 360 ,loss = 0.15059643983840942 , training accuracy = tensor(0.9780)\n",
      "epoch: 370 ,loss = 0.14826974272727966 , training accuracy = tensor(0.9790)\n",
      "epoch: 380 ,loss = 0.14602316915988922 , training accuracy = tensor(0.9790)\n",
      "epoch: 390 ,loss = 0.14385224878787994 , training accuracy = tensor(0.9800)\n",
      "epoch: 400 ,loss = 0.14175280928611755 , training accuracy = tensor(0.9800)\n",
      "epoch: 410 ,loss = 0.13972100615501404 , training accuracy = tensor(0.9800)\n",
      "epoch: 420 ,loss = 0.13775330781936646 , training accuracy = tensor(0.9810)\n",
      "epoch: 430 ,loss = 0.13584642112255096 , training accuracy = tensor(0.9820)\n",
      "epoch: 440 ,loss = 0.13399729132652283 , training accuracy = tensor(0.9820)\n",
      "epoch: 450 ,loss = 0.1322030872106552 , training accuracy = tensor(0.9820)\n",
      "epoch: 460 ,loss = 0.1304611712694168 , training accuracy = tensor(0.9820)\n",
      "epoch: 470 ,loss = 0.12876906991004944 , training accuracy = tensor(0.9830)\n",
      "epoch: 480 ,loss = 0.12712450325489044 , training accuracy = tensor(0.9840)\n",
      "epoch: 490 ,loss = 0.12552528083324432 , training accuracy = tensor(0.9840)\n",
      "epoch: 500 ,loss = 0.1239694282412529 , training accuracy = tensor(0.9840)\n",
      "epoch: 510 ,loss = 0.12245501577854156 , training accuracy = tensor(0.9860)\n",
      "epoch: 520 ,loss = 0.12098028510808945 , training accuracy = tensor(0.9860)\n",
      "epoch: 530 ,loss = 0.11954355984926224 , training accuracy = tensor(0.9860)\n",
      "epoch: 540 ,loss = 0.11814325302839279 , training accuracy = tensor(0.9870)\n",
      "epoch: 550 ,loss = 0.11677791178226471 , training accuracy = tensor(0.9870)\n",
      "epoch: 560 ,loss = 0.11544613540172577 , training accuracy = tensor(0.9870)\n",
      "epoch: 570 ,loss = 0.11414659023284912 , training accuracy = tensor(0.9870)\n",
      "epoch: 580 ,loss = 0.11287803202867508 , training accuracy = tensor(0.9870)\n",
      "epoch: 590 ,loss = 0.11163929104804993 , training accuracy = tensor(0.9880)\n",
      "epoch: 600 ,loss = 0.11042924225330353 , training accuracy = tensor(0.9890)\n",
      "epoch: 610 ,loss = 0.10924682766199112 , training accuracy = tensor(0.9890)\n",
      "epoch: 620 ,loss = 0.10809103399515152 , training accuracy = tensor(0.9900)\n",
      "epoch: 630 ,loss = 0.10696092247962952 , training accuracy = tensor(0.9900)\n",
      "epoch: 640 ,loss = 0.10585557669401169 , training accuracy = tensor(0.9900)\n",
      "epoch: 650 ,loss = 0.10477414727210999 , training accuracy = tensor(0.9900)\n",
      "epoch: 660 ,loss = 0.10371581465005875 , training accuracy = tensor(0.9900)\n",
      "epoch: 670 ,loss = 0.10267980396747589 , training accuracy = tensor(0.9900)\n",
      "epoch: 680 ,loss = 0.10166534781455994 , training accuracy = tensor(0.9910)\n",
      "epoch: 690 ,loss = 0.10067174583673477 , training accuracy = tensor(0.9910)\n",
      "epoch: 700 ,loss = 0.09969832748174667 , training accuracy = tensor(0.9910)\n",
      "epoch: 710 ,loss = 0.09874444454908371 , training accuracy = tensor(0.9910)\n",
      "epoch: 720 ,loss = 0.09780946373939514 , training accuracy = tensor(0.9910)\n",
      "epoch: 730 ,loss = 0.0968928411602974 , training accuracy = tensor(0.9910)\n",
      "epoch: 740 ,loss = 0.09599395841360092 , training accuracy = tensor(0.9920)\n",
      "epoch: 750 ,loss = 0.09511230140924454 , training accuracy = tensor(0.9920)\n",
      "epoch: 760 ,loss = 0.09424737840890884 , training accuracy = tensor(0.9930)\n",
      "epoch: 770 ,loss = 0.09339863061904907 , training accuracy = tensor(0.9930)\n",
      "epoch: 780 ,loss = 0.0925656259059906 , training accuracy = tensor(0.9930)\n",
      "epoch: 790 ,loss = 0.09174789488315582 , training accuracy = tensor(0.9930)\n",
      "epoch: 800 ,loss = 0.09094500541687012 , training accuracy = tensor(0.9930)\n",
      "epoch: 810 ,loss = 0.09015653282403946 , training accuracy = tensor(0.9940)\n",
      "epoch: 820 ,loss = 0.08938208222389221 , training accuracy = tensor(0.9940)\n",
      "epoch: 830 ,loss = 0.08862124383449554 , training accuracy = tensor(0.9940)\n",
      "epoch: 840 ,loss = 0.0878736674785614 , training accuracy = tensor(0.9940)\n",
      "epoch: 850 ,loss = 0.08713898062705994 , training accuracy = tensor(0.9940)\n",
      "epoch: 860 ,loss = 0.0864168331027031 , training accuracy = tensor(0.9940)\n",
      "epoch: 870 ,loss = 0.08570690453052521 , training accuracy = tensor(0.9940)\n",
      "epoch: 880 ,loss = 0.08500886708498001 , training accuracy = tensor(0.9940)\n",
      "epoch: 890 ,loss = 0.08432242274284363 , training accuracy = tensor(0.9940)\n",
      "epoch: 900 ,loss = 0.08364726603031158 , training accuracy = tensor(0.9940)\n",
      "epoch: 910 ,loss = 0.0829830914735794 , training accuracy = tensor(0.9940)\n",
      "epoch: 920 ,loss = 0.0823296383023262 , training accuracy = tensor(0.9940)\n",
      "epoch: 930 ,loss = 0.08168665319681168 , training accuracy = tensor(0.9960)\n",
      "epoch: 940 ,loss = 0.08105385303497314 , training accuracy = tensor(0.9960)\n",
      "epoch: 950 ,loss = 0.0804310068488121 , training accuracy = tensor(0.9960)\n",
      "epoch: 960 ,loss = 0.07981785386800766 , training accuracy = tensor(0.9970)\n",
      "epoch: 970 ,loss = 0.0792141705751419 , training accuracy = tensor(0.9970)\n",
      "epoch: 980 ,loss = 0.07861972600221634 , training accuracy = tensor(0.9970)\n",
      "epoch: 990 ,loss = 0.07803432643413544 , training accuracy = tensor(0.9980)\n",
      "epoch: 1000 ,loss = 0.0774577334523201 , training accuracy = tensor(0.9980)\n",
      "epoch: 1010 ,loss = 0.0768897533416748 , training accuracy = tensor(0.9980)\n",
      "epoch: 1020 ,loss = 0.07633017003536224 , training accuracy = tensor(0.9980)\n",
      "epoch: 1030 ,loss = 0.07577882707118988 , training accuracy = tensor(0.9990)\n",
      "epoch: 1040 ,loss = 0.07523550093173981 , training accuracy = tensor(0.9990)\n",
      "epoch: 1050 ,loss = 0.07470004260540009 , training accuracy = tensor(0.9990)\n",
      "epoch: 1060 ,loss = 0.07417226582765579 , training accuracy = tensor(0.9990)\n",
      "epoch: 1070 ,loss = 0.0736519917845726 , training accuracy = tensor(0.9990)\n",
      "epoch: 1080 ,loss = 0.07313908636569977 , training accuracy = tensor(0.9990)\n",
      "epoch: 1090 ,loss = 0.07263334840536118 , training accuracy = tensor(0.9990)\n",
      "epoch: 1100 ,loss = 0.07213465869426727 , training accuracy = tensor(0.9990)\n",
      "epoch: 1110 ,loss = 0.07164284586906433 , training accuracy = tensor(0.9990)\n",
      "epoch: 1120 ,loss = 0.0711577758193016 , training accuracy = tensor(0.9990)\n",
      "epoch: 1130 ,loss = 0.07067930698394775 , training accuracy = tensor(1.)\n",
      "epoch: 1140 ,loss = 0.07020729035139084 , training accuracy = tensor(1.)\n",
      "epoch: 1150 ,loss = 0.06974160671234131 , training accuracy = tensor(1.)\n",
      "epoch: 1160 ,loss = 0.06928212195634842 , training accuracy = tensor(1.)\n",
      "epoch: 1170 ,loss = 0.06882870197296143 , training accuracy = tensor(1.)\n",
      "epoch: 1180 ,loss = 0.06838124990463257 , training accuracy = tensor(1.)\n",
      "epoch: 1190 ,loss = 0.06793961673974991 , training accuracy = tensor(1.)\n",
      "epoch: 1200 ,loss = 0.06750369071960449 , training accuracy = tensor(1.)\n",
      "epoch: 1210 ,loss = 0.06707337498664856 , training accuracy = tensor(1.)\n",
      "epoch: 1220 ,loss = 0.06664855033159256 , training accuracy = tensor(1.)\n",
      "epoch: 1230 ,loss = 0.06622911244630814 , training accuracy = tensor(1.)\n",
      "epoch: 1240 ,loss = 0.06581495702266693 , training accuracy = tensor(1.)\n",
      "epoch: 1250 ,loss = 0.0654059648513794 , training accuracy = tensor(1.)\n",
      "epoch: 1260 ,loss = 0.06500207632780075 , training accuracy = tensor(1.)\n",
      "epoch: 1270 ,loss = 0.06460316479206085 , training accuracy = tensor(1.)\n",
      "epoch: 1280 ,loss = 0.06420915573835373 , training accuracy = tensor(1.)\n",
      "epoch: 1290 ,loss = 0.06381993740797043 , training accuracy = tensor(1.)\n",
      "epoch: 1300 ,loss = 0.06343542784452438 , training accuracy = tensor(1.)\n",
      "epoch: 1310 ,loss = 0.06305554509162903 , training accuracy = tensor(1.)\n",
      "epoch: 1320 ,loss = 0.0626802071928978 , training accuracy = tensor(1.)\n",
      "epoch: 1330 ,loss = 0.062309324741363525 , training accuracy = tensor(1.)\n",
      "epoch: 1340 ,loss = 0.061942823231220245 , training accuracy = tensor(1.)\n",
      "epoch: 1350 ,loss = 0.06158062070608139 , training accuracy = tensor(1.)\n",
      "epoch: 1360 ,loss = 0.06122264266014099 , training accuracy = tensor(1.)\n",
      "epoch: 1370 ,loss = 0.06086880341172218 , training accuracy = tensor(1.)\n",
      "epoch: 1380 ,loss = 0.060519054532051086 , training accuracy = tensor(1.)\n",
      "epoch: 1390 ,loss = 0.06017330288887024 , training accuracy = tensor(1.)\n",
      "epoch: 1400 ,loss = 0.059831488877534866 , training accuracy = tensor(1.)\n",
      "epoch: 1410 ,loss = 0.059493537992239 , training accuracy = tensor(1.)\n",
      "epoch: 1420 ,loss = 0.05915939435362816 , training accuracy = tensor(1.)\n",
      "epoch: 1430 ,loss = 0.05882897973060608 , training accuracy = tensor(1.)\n",
      "epoch: 1440 ,loss = 0.05850224196910858 , training accuracy = tensor(1.)\n",
      "epoch: 1450 ,loss = 0.058179114013910294 , training accuracy = tensor(1.)\n",
      "epoch: 1460 ,loss = 0.05785954371094704 , training accuracy = tensor(1.)\n",
      "epoch: 1470 ,loss = 0.05754345655441284 , training accuracy = tensor(1.)\n",
      "epoch: 1480 ,loss = 0.05723080784082413 , training accuracy = tensor(1.)\n",
      "epoch: 1490 ,loss = 0.05692153424024582 , training accuracy = tensor(1.)\n",
      "epoch: 1500 ,loss = 0.05661558359861374 , training accuracy = tensor(1.)\n",
      "epoch: 1510 ,loss = 0.056312911212444305 , training accuracy = tensor(1.)\n",
      "epoch: 1520 ,loss = 0.056013450026512146 , training accuracy = tensor(1.)\n",
      "epoch: 1530 ,loss = 0.05571715161204338 , training accuracy = tensor(1.)\n",
      "epoch: 1540 ,loss = 0.05542396381497383 , training accuracy = tensor(1.)\n",
      "epoch: 1550 ,loss = 0.055133845657110214 , training accuracy = tensor(1.)\n",
      "epoch: 1560 ,loss = 0.05484674125909805 , training accuracy = tensor(1.)\n",
      "epoch: 1570 ,loss = 0.054562609642744064 , training accuracy = tensor(1.)\n",
      "epoch: 1580 ,loss = 0.05428140610456467 , training accuracy = tensor(1.)\n",
      "epoch: 1590 ,loss = 0.054003067314624786 , training accuracy = tensor(1.)\n",
      "epoch: 1600 ,loss = 0.053727567195892334 , training accuracy = tensor(1.)\n",
      "epoch: 1610 ,loss = 0.05345485731959343 , training accuracy = tensor(1.)\n",
      "epoch: 1620 ,loss = 0.05318489298224449 , training accuracy = tensor(1.)\n",
      "epoch: 1630 ,loss = 0.052917640656232834 , training accuracy = tensor(1.)\n",
      "epoch: 1640 ,loss = 0.05265304446220398 , training accuracy = tensor(1.)\n",
      "epoch: 1650 ,loss = 0.05239107832312584 , training accuracy = tensor(1.)\n",
      "epoch: 1660 ,loss = 0.052131690084934235 , training accuracy = tensor(1.)\n",
      "epoch: 1670 ,loss = 0.051874853670597076 , training accuracy = tensor(1.)\n",
      "epoch: 1680 ,loss = 0.05162052810192108 , training accuracy = tensor(1.)\n",
      "epoch: 1690 ,loss = 0.05136867240071297 , training accuracy = tensor(1.)\n",
      "epoch: 1700 ,loss = 0.051119256764650345 , training accuracy = tensor(1.)\n",
      "epoch: 1710 ,loss = 0.050872236490249634 , training accuracy = tensor(1.)\n",
      "epoch: 1720 ,loss = 0.050627581775188446 , training accuracy = tensor(1.)\n",
      "epoch: 1730 ,loss = 0.05038526654243469 , training accuracy = tensor(1.)\n",
      "epoch: 1740 ,loss = 0.05014524608850479 , training accuracy = tensor(1.)\n",
      "epoch: 1750 ,loss = 0.049907486885786057 , training accuracy = tensor(1.)\n",
      "epoch: 1760 ,loss = 0.0496719665825367 , training accuracy = tensor(1.)\n",
      "epoch: 1770 ,loss = 0.04943865165114403 , training accuracy = tensor(1.)\n",
      "epoch: 1780 ,loss = 0.04920750483870506 , training accuracy = tensor(1.)\n",
      "epoch: 1790 ,loss = 0.04897849261760712 , training accuracy = tensor(1.)\n",
      "epoch: 1800 ,loss = 0.0487515963613987 , training accuracy = tensor(1.)\n",
      "epoch: 1810 ,loss = 0.04852678254246712 , training accuracy = tensor(1.)\n",
      "epoch: 1820 ,loss = 0.04830402508378029 , training accuracy = tensor(1.)\n",
      "epoch: 1830 ,loss = 0.048083290457725525 , training accuracy = tensor(1.)\n",
      "epoch: 1840 ,loss = 0.04786454513669014 , training accuracy = tensor(1.)\n",
      "epoch: 1850 ,loss = 0.04764777794480324 , training accuracy = tensor(1.)\n",
      "epoch: 1860 ,loss = 0.047432951629161835 , training accuracy = tensor(1.)\n",
      "epoch: 1870 ,loss = 0.04722003638744354 , training accuracy = tensor(1.)\n",
      "epoch: 1880 ,loss = 0.047009024769067764 , training accuracy = tensor(1.)\n",
      "epoch: 1890 ,loss = 0.04679987579584122 , training accuracy = tensor(1.)\n",
      "epoch: 1900 ,loss = 0.04659256339073181 , training accuracy = tensor(1.)\n",
      "epoch: 1910 ,loss = 0.046387072652578354 , training accuracy = tensor(1.)\n",
      "epoch: 1920 ,loss = 0.046183373779058456 , training accuracy = tensor(1.)\n",
      "epoch: 1930 ,loss = 0.04598144441843033 , training accuracy = tensor(1.)\n",
      "epoch: 1940 ,loss = 0.045781269669532776 , training accuracy = tensor(1.)\n",
      "epoch: 1950 ,loss = 0.04558280482888222 , training accuracy = tensor(1.)\n",
      "epoch: 1960 ,loss = 0.04538604989647865 , training accuracy = tensor(1.)\n",
      "epoch: 1970 ,loss = 0.04519098252058029 , training accuracy = tensor(1.)\n",
      "epoch: 1980 ,loss = 0.04499756172299385 , training accuracy = tensor(1.)\n",
      "epoch: 1990 ,loss = 0.04480578005313873 , training accuracy = tensor(1.)\n",
      "epoch: 2000 ,loss = 0.044615618884563446 , training accuracy = tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "bow_nn_model = train(bow_nn_model, X_TRAIN, Y_TRAIN, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4D7NpQlkBGf"
   },
   "source": [
    "# Logistic Regression - Part 2 Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OoOf1CcWIKT-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bow_nn_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in TEST.iterrows():\n",
    "        bow_vec = make_bow(row['review'])\n",
    "        probs = bow_nn_model(bow_vec.float().to(device))\n",
    "        pred = 1 if probs[0][0] > 0.5 else 0\n",
    "        bow_nn_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qW0uESVguCuL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      1000\n",
      "           1       0.80      0.82      0.81      1000\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.80      0.80      2000\n",
      "weighted avg       0.81      0.81      0.80      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.805"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student check - accuracies >= 80% will receive full credit (no credit for less than 80%)\n",
    "print(classification_report(TEST['label'],bow_nn_predictions))\n",
    "(bow_nn_predictions==TEST['label']).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LD3y8LCKhH74",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26b59c71c98aa74a451480e472e92934",
     "grade": true,
     "grade_id": "cell-cd5c01945e7e8f55",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DphmlDCk5sr5"
   },
   "source": [
    "# Multinomial Regression\n",
    "\n",
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SIiy4PBJVh7h",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\jaewo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaewo\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jaewo\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jaewo\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GrcwUiN-VfSf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaewo\\anaconda3\\envs\\assign-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqcveFex52Fy"
   },
   "source": [
    "Unlike earlier, we will use a pre-defined set of embeddings, called [GLoVe](https://nlp.stanford.edu/projects/glove/). GLoVe replaces every word with a 100-dimensional vector of floating point values. The advantage of this is that words with similar semantic meanings will have similar vectors. This is important because the vocabulary size of the corpus we will use is 400,000.\n",
    "\n",
    "For the assigment, instead of getting a one-hot vector for each word, the neural network will get a `batch_size x num_words x 100` tensor containing floating point values.\n",
    "\n",
    "Download the GLoVe embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7zIXPea4pZCi"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-_CNat0Qpf_b"
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "VOCAB_SIZE = len(glove_vectors.vectors)\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRcY0APp-9YI"
   },
   "source": [
    "This function will embed the dataset into sequences of 100-dimension vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Yo6wUdb9s97G"
   },
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "MAX_LEN = 50\n",
    "\n",
    "def get_glove_seq(review, max_len):\n",
    "  seq = np.zeros((max_len, 100))\n",
    "  for i, word in enumerate(review):\n",
    "    if i < max_len and word in glove_vectors:\n",
    "      seq[i] = glove_vectors[word]\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ZtpvG4mJnwMf"
   },
   "outputs": [],
   "source": [
    "news_data_train = load_dataset(\"ag_news\", split=\"train\").shuffle()\n",
    "news_data_test = load_dataset(\"ag_news\", split=\"test\").shuffle()\n",
    "NEWS_TRAIN = pd.DataFrame(news_data_train)[:5000]\n",
    "NEWS_TEST = pd.DataFrame(news_data_test)[:5000]\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pmx1OElZaAki"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lacie announces external SATA harddrive, high-...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Palestinians Sift Rubble After Israel's Gaza A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Microsoft issues patches for 7 software flaws ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bobcats Trade Drobnjak to Hawks for Pick (AP) ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dover #39;s place in  #39;Chase #39; looks sec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Lacie announces external SATA harddrive, high-...      3\n",
       "1  Palestinians Sift Rubble After Israel's Gaza A...      0\n",
       "2  Microsoft issues patches for 7 software flaws ...      3\n",
       "3  Bobcats Trade Drobnjak to Hawks for Pick (AP) ...      1\n",
       "4  Dover #39;s place in  #39;Chase #39; looks sec...      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEWS_TEST.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5IzjEK0Ixxl"
   },
   "source": [
    "Train/Test Sets using GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "-I8PSBk-I6qM"
   },
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers\n",
    "X_NEWS_TRAIN = np.zeros((len(NEWS_TRAIN), MAX_LEN, 100))\n",
    "X_NEWS_TEST = np.zeros((len(NEWS_TEST), MAX_LEN, 100))\n",
    "\n",
    "for i, row in NEWS_TRAIN.iterrows():\n",
    "  X_NEWS_TRAIN[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "for i, row in NEWS_TEST.iterrows():\n",
    "  X_NEWS_TEST[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "Y_NEWS_TRAIN = np.array(NEWS_TRAIN['label'])\n",
    "Y_NEWS_TEST = np.array(NEWS_TEST['label'])\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "id": "i1-mK1-EPbQL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a2701249da06764a91a990e6d94edd",
     "grade": false,
     "grade_id": "cell-64776db849e2cffa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class MultinomialBoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "  def __init__(self, max_word_len, embedding_dim, num_labels):\n",
    "    super(MultinomialBoWClassifier, self).__init__()\n",
    "    self.max_word_len = max_word_len\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_labels = num_labels\n",
    "    # YOUR CODE HERE\n",
    "    self.linear = nn.Linear(max_word_len * embedding_dim, num_labels)\n",
    "\n",
    "\n",
    "  def forward(self, x): \n",
    "    out = None\n",
    "    # YOUR CODE HERE\n",
    "    x = x.view(-1, self.max_word_len * self.embedding_dim)\n",
    "    out = self.linear(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "_eVavHp7TOxv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialBoWClassifier(\n",
       "  (linear): Linear(in_features=5000, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multibow_model = MultinomialBoWClassifier(max_word_len=MAX_LEN, embedding_dim=EMBEDDING_DIM, num_labels=NUM_LABELS)\n",
    "multibow_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "POVLJlC0_mEl"
   },
   "outputs": [],
   "source": [
    "# It's ok to modify this cell.\n",
    "\n",
    "# 0.7605\n",
    "# BATCH_SIZE = 50\n",
    "# N_EPOCHS = 100\n",
    "# LEARNING_RATE = 5e-4\n",
    "# WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# 0.795\n",
    "BATCH_SIZE = 10\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 6e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# 0.7174 \n",
    "# BATCH_SIZE = 50\n",
    "# N_EPOCHS = 100\n",
    "# LEARNING_RATE = 5e-3\n",
    "# WEIGHT_DECAY = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "wc0JD91dP9-a"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, x_train_data, y_train_data, epochs, batch_size, lr, weight_decay):\n",
    "  print('Training Started!')\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  n_iter = len(x_train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    num_correct = 0\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, x_train_data, y_train_data)\n",
    "      x = x.to(device)\n",
    "      y = y.long().to(device)\n",
    "\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      total_loss += loss\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(y_hat.argmax(dim=1)==y).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "uhVQB2RCbPj7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "500 batches per epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 ,loss = 0.0011953325010836124 , training accuracy = 1.0\n",
      "epoch: 20 ,loss = 0.000563891779165715 , training accuracy = 1.0\n",
      "epoch: 30 ,loss = 0.0003528358938638121 , training accuracy = 1.0\n",
      "epoch: 40 ,loss = 0.0003203710657544434 , training accuracy = 1.0\n",
      "epoch: 50 ,loss = 0.00022535216703545302 , training accuracy = 1.0\n",
      "epoch: 60 ,loss = 0.0004405081272125244 , training accuracy = 1.0\n",
      "epoch: 70 ,loss = 0.00028884364292025566 , training accuracy = 1.0\n",
      "epoch: 80 ,loss = 0.0006320413085632026 , training accuracy = 1.0\n",
      "epoch: 90 ,loss = 0.0003375050437171012 , training accuracy = 1.0\n",
      "epoch: 100 ,loss = 0.0004328397917561233 , training accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "train(multibow_model, X_NEWS_TRAIN, Y_NEWS_TRAIN, N_EPOCHS, BATCH_SIZE, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50_KqANXCtbs"
   },
   "source": [
    "# Multinomial Regression - Test (40 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "e9ENFPhRIKUF"
   },
   "outputs": [],
   "source": [
    "multibow_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_vec = torch.tensor(X_NEWS_TEST, dtype=torch.float, device=device)\n",
    "    probs = multibow_model(text_vec)\n",
    "    pred = probs.argmax(dim=1)\n",
    "\n",
    "targets = torch.tensor(Y_NEWS_TEST, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Ltz2NZ_4u9Cr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022000193595886"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student check - accuracies >= 80% will receive full credit (no credit for less than 80%)\n",
    "(pred==targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FFtKpTdSuyxy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac6d1603fc26687666de3e39352cf627",
     "grade": true,
     "grade_id": "cell-e489259c0e7f9377",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hidden auto-grader test - do not delete this cell '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Hidden auto-grader test - do not delete this cell '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKEYvlbhS7n9"
   },
   "source": [
    "# Grading\n",
    "\n",
    "Please submit this .ipynb file to Canvas for grading."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "captumWidgetMessage": {},
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1TsKPn8cwghhFR-TwD0la25YRRzhw4XF9",
     "timestamp": 1685998969758
    }
   ]
  },
  "dataExplorerConfig": {},
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "last_base_url": "https://11238.od.fbinfra.net/",
  "last_kernel_id": "8832c409-9272-44ac-a889-c233991c3bb0",
  "last_msg_id": "354bb3e2-f350a4bd64779ec7f6ce4382_4332",
  "last_server_session_id": "5625d4d2-18a1-473d-bf89-0f1a5ecc7c7b",
  "outputWidgetContext": {}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
